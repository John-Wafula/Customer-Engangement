---
title: "Text Clustering"
author: '...'
date: "12/18/2021"
output: html_document
---

```{r}
## CLUSTERING
## Customer Engagement Facebook - Thailand

live = read.csv("C:/Users/Admin/Downloads/Live.csv")

## DAtaSet Check

dim(live)

colSums(is.na(live))             ## Check missing Values - NAs

live = live[,-c(13,14,15,16)]     ## Remove un-necessary vars with missing NAs

str(live)
summary(live)

live = live[,-1]           ## Remove status_id var -

```
```{r}
## Convert Factor into Date 

attach(live)

library(lubridate)
library(dplyr)

live$status_published = mdy_hm(as.character(live$status_published))


write.csv(live, 'live_fb.csv')


## Used Excel to Seprate Data and Time 

## Load data

live_fb = read.csv("C:/Users/Admin/Downloads/live_fb.csv")

attach(live_fb)
```
```{r}
library(tidyr)
live_fb = live_fb %>% separate(status_published, sep="-", 
                               into = c("status_day", "status_month", "status_year"))

write.csv(live_fb, 'live_fb.csv')
```


```{r}
## Data Analysis & EDA

live_fb = read.csv('live_fb.csv')

attach(live_fb)


library(rpivotTable)

rpivotTable(live_fb)
```
```{r}
#Value      Bin       
0-30   ->  Low       
31-70  ->  Mid       
71-100 ->  High

#num_reaction is sum of - num_likes, num_loves, num_wows, num_hahas, num_sads and num_angrys





live_fb = live_fb[,-c(1,6)]     ## Remove Factor status_type & num_reaction

```

```{r}
## Clustering Tendency - Hopkins Statistics : 0 to 1

library(factoextra)

n<-get_clust_tendency(live_fb, graph = T, n=5, seed = 123)
n
```


```{r}
# Here 'n' is random pickup of dataset for hopkins Stats - to check cluster tendency

 hopkins_stat
# [1] 0.01066148

# Near to zero indicated presence of valid clusters 
```


```{r}
## Build K-Means Clustering 

library(NbClust)

ind = sample(2, nrow(live_fb), replace = TRUE, prob = c(0.3,0.7))

test1 = live_fb[ind == 1, ]

cluster1 = NbClust(test1, distance = 'euclidean', method = 'kmeans')

kmeans.c = kmeans(test1, centers = 2, nstart = 50)

print(kmeans.c)
```




```{r}
## Plot Cluster

library(factoextra)

print(fviz_cluster(kmeans.c, data = test1))


## FINAL STEP
## Cluster Performance Measure - with 'silhouette' between -1 to 1

si = silhouette(kmeans.c$cluster, dist(test1, 'euclidean'))

print(summary(si))

# Cluster sizes and average silhouette widths:
#   51      2107 
# 0.4006616 0.8417667 

## NOTE: Values might chnge due to sampling in var test1 
```

When comments are less than ten, the reaction to comment ratio becomes higher. It means it created impressions but may not be enough interest in the customer base. At the same time, we can see that the three interaction types in the data 'haha', 'angry,' and 'sad' are there. Knowing Facebook as a social platform, these reactions are expressed in extreme emotions or disappointed by the product. It is work exploring the positive reactions 'likes,' 'loves,' and 'wows.'

With the variables mentioned above, we can check if the reaction to comment ratio is higher for selling attempts with positive comments or negative comments.

We found the following data quality issues and appropriate remedy implemented.

1) Duplicate records - There were 53 records duplicated, and we preserved the last records.

2) Calculated Columns value Mismatch - The column num_reactions column is created by summing the columns num_likes, num_loves,num_wows,num_hahas,num_sads,num_angrys. There was nine instanced where the values are not matching. The values were replaced with correct calculations.






One interesting factor that can be explored form the data is the impact of time in engagements. From the status_published, we can create a segment of a day such as early morning, morning, noon, evening, and night. Another new variable idea will be the days, such as working day or weekend. Generally, Facebook live sellers may be trying to earn a side income and selecting free time from work. The sellers may have a hunch on the best time they are getting audience and revenue. By creating this variable, we may be able to frame a use case. One can create a couple of new Machine Learning use cases from this new data.











































